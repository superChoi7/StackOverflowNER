{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "- Authors implement a custom tokenizer because existing tokenizers: CMU Twokenizer, Stanford TweetTokenizer, NLTK twitter tokenizer all mistakenly split code, for example:\n",
    "```\n",
    "txScope.complete() => [\"txScope\", \".\", \"complete\", \"(\", \")\"]\n",
    "std::condition_variable => [\"std\", \":\", \":\", \"condition_variable\"]\n",
    "math.h => [\"math\", \".\", \"h\"]\n",
    "<html> => [\"<\", \"html\", \">\"]\n",
    "a == b => [\"a\", \"=\", \"=\", \"b\"]\n",
    "```\n",
    "- Ambiguity, hard to refer word to technical programming concepts or common languages, such as \"go\", \"spring\", \"while\", \"if\", \"select\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture\n",
    "1. **Input embedding layer**: Extract contextualized embeddings from the $BERT_{base}$ model and two new domain-specific embeddings for each word in the input sentence.\n",
    "2. **Embedding attention layer**: Combine the three word embeddings using an attention network.\n",
    "3. **Linear-CRF layer**: Predict the entity type of each word using the attentive word representations from the previous layer.\n",
    "![model.png](./notebook_resources/model.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input embeddings\n",
    "\n",
    "1. **Code Recognizer**, which represents if a word can be part of a code entity regardless of context\n",
    "2. **Entity Segmenter**, that predicts whether a word is part of any named entity in the given sentence.\n",
    "\n",
    "#### In-domain Word Embeddings\n",
    "Wikipedia text is unsuitable for technical context, StackOverflow 10-year archive is used in this task.\n",
    "**BERT (BERTOverflow)**, **ELMo (ELMoVerflow)**, and **GloVe (GloVerflow)** are trained.\n",
    "\n",
    "#### Context-independent Code Recognition\n",
    "Because `list` can be either common english word or code snippet, while `listing` is unlikely to be a code snippet.\n",
    "Thus, a a binary classfier code recognition moodule is used for this task regardless of the context.\n",
    "\n",
    "- The input features include unigram word and 6-gram character probabilities from two language models that are trained on the **Gigaword corpus** and all the code-snippets in the **StackOverflow 10-year archive**\n",
    "- Pre-trained **FastText word embeddings** using these code-snippets, where a word vector is represented as a sum of its character ngrams. We first transform each ngram probability into a k-dimensional vector using Gaussian binning.\n",
    "- then feed the vectorized features into a linear layer, concatenate the output with FastText character-level embeddings\n",
    "- pass them through another hidden layer with sigmoid activation, and see if the output probability is greater than 0.5\n",
    "\n",
    "#### Entity segmentation\n",
    "- **Word frequency**: the word occurrence count in the training set.\n",
    "\n",
    "In the giving context(Stackoverflow), code and non-code have an average frequency of 1.47 and 7.41. Ambiguous token that can be either code or non-code entities, such as \"windows\", have a much higher average frequency of 92.57\n",
    "- **Code markdown**: whether the given token appears inside a ⟨code⟩ markdown in the StackOverflow post.\n",
    "\n",
    "This is noisy as users do not always enclose inline code in a ⟨code⟩ tag or sometimes use the tag to highlight non-code texts \n",
    "\n",
    "#### Embedding-Level Attention\n",
    "For each word $w_i$, there are 3 embeddings, BERT ($w_{i1}$), Code recognizer ($w_{i2}$), Entity Segmenter ($w_{i3}$). The embedding-level attention $\\alpha_{it}$ ($t \\in \\{1, 2, 3\\}$) capture the word's contribution to the meaning of the word.\n",
    "\n",
    "To compute $\\alpha_{it}$, we pass the input embeddings through a bidirectional GRU and generate their corresponding hidden representations $h_{it} = \\vec{GRU}(w_{it})$\n",
    "\n",
    "These vectors are then passed through a non-linear layer, which outputs $u_{it} = tanh(W_e h_{it} + b_e)$.\n",
    "\n",
    "$u_e$: randomly initialized and updated during the training process.\n",
    "\n",
    "This context vector is combined with the hidden embedding representation using a softmax function to extract weight of the embeddings:\n",
    "$$\n",
    "  \\alpha_{it} = \\frac{\\exp{u_{it}^T u_e}}{\\sum_t \\exp{u_{it}^T u_e}}\n",
    "$$\n",
    "\n",
    "Finally, we create the word vector by a weighted sum of all the information from different embeddings as \n",
    "$$\n",
    "  word_i = \\sum_t \\alpha_{it}h_{it}\n",
    "$$\n",
    "\n",
    "The result is then fed into a linear-CRF layer, which predicts the entity category for each word based the BIO tagging schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train SoftNER model and 2 auxiliary model separately. Segmentation model follows the simple BERT fine-tuning architecture except for the input, where BERT embeddings are concatenated with 100-dimensional code markdown and 10-dimensional word frequency features.\n",
    "\n",
    "Number of bins $k = 10$ for Gaussian vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "- [Pretrained BERTOverflow](https://github.com/lanwuwei/BERTOverflow)\n",
    "- [BertOverflow weights and words](https://drive.google.com/drive/folders/1z4zXexpYU10QNlpcSA_UPfMb2V34zHHO)\n",
    "### 20 entities\n",
    "#### 8 code entities\n",
    "- CLASS\n",
    "- VARIABLE\n",
    "- IN LINE CODE\n",
    "- FUNCTION\n",
    "- LIBRARY\n",
    "- VALUE\n",
    "- DATA TYPE\n",
    "- HTML XML TAG\n",
    "\n",
    "#### 12 natural language entities\n",
    "- APPLICATION\n",
    "- UI ELEMENT\n",
    "- LANGUAGE\n",
    "- DATA STRUCTURE\n",
    "- ALGORITHM\n",
    "- FILE TYPE\n",
    "- FILE NAME\n",
    "- VERSION\n",
    "- DEVICE\n",
    "- OS\n",
    "- WEBSITE\n",
    "- USER NAME"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
